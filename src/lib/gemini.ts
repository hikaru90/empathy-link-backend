/**
 * Gemini AI client for chat functionality
 */

import { GoogleGenAI } from '@google/genai';
import type { HistoryEntry } from './encryption.js';
import { CONVERSATION_PATHS } from './paths.js';

// Initialize Gemini AI client
let aiClient: GoogleGenAI | null = null;

function getAiClient(): GoogleGenAI {
	if (!aiClient) {
		const apiKey = process.env.GEMINI_API_KEY;
		if (!apiKey) {
			throw new Error('GEMINI_API_KEY environment variable is required');
		}
		aiClient = new GoogleGenAI({ apiKey });
	}
	return aiClient;
}

// Export the client for use in other modules
export { getAiClient };

// Path Switching Analysis Types
export type PathSwitchAnalysis = {
	shouldSwitch: boolean;
	confidence: number;
	suggestedPath: string | null;
	reason: string;
	currentPathComplete: boolean;
};

/**
 * Convert database history to Gemini format with sliding window
 * Filters out path markers and keeps only recent conversational messages
 */
export function convertHistoryToGemini(dbHistory: HistoryEntry[], maxMessages: number = 20) {
	const filtered = dbHistory
		// Filter out path markers and other non-conversational entries
		.filter(entry =>
			!entry.pathMarker &&
			!entry.hidden &&
			entry.role &&
			entry.parts &&
			entry.parts[0]?.text &&
			entry.parts[0].text.trim() !== ''
		)
		// Convert to Gemini format
		.map(entry => ({
			role: entry.role,
			parts: entry.parts
		}));

	// Implement sliding window: keep only the most recent messages
	if (filtered.length <= maxMessages) {
		// Ensure first message is from user (Gemini requirement)
		if (filtered.length > 0 && filtered[0].role !== 'user') {
			const firstUserIndex = filtered.findIndex(msg => msg.role === 'user');
			if (firstUserIndex > 0) {
				return filtered.slice(firstUserIndex);
			}
		}
		return filtered;
	}

	const recentMessages = filtered.slice(-maxMessages);

	// Ensure first message is from user (Gemini requirement)
	if (recentMessages.length > 0 && recentMessages[0].role !== 'user') {
		const firstUserIndex = recentMessages.findIndex(msg => msg.role === 'user');
		if (firstUserIndex > 0) {
			return recentMessages.slice(firstUserIndex);
		}
	}

	return recentMessages;
}

/**
 * Send a message to Gemini and get AI response
 */
export async function getAiResponse(
	message: string,
	history: HistoryEntry[],
	systemInstruction: string
): Promise<string> {
	try {
		const ai = getAiClient();

		// Convert history to Gemini format
		const geminiHistory = convertHistoryToGemini(history);

		console.log('Sending to Gemini:', {
			messageLength: message.length,
			historyLength: geminiHistory.length,
			systemInstructionLength: systemInstruction.length
		});

		// Create chat with system instruction and history
		const chat = ai.chats.create({
			model: 'gemini-2.5-flash',
			config: {
				temperature: 0.7,
				topP: 0.95,
				topK: 64,
				maxOutputTokens: 8192,
				systemInstruction
			},
			history: geminiHistory
		});

		// Send the new message
		const result = await chat.sendMessage({ message });

		const responseText = result.text || '';

		console.log('Gemini response received:', {
			responseLength: responseText.length,
			truncated: responseText.substring(0, 100) + '...'
		});

		return responseText;

	} catch (error) {
		console.error('Error getting AI response:', error);

		// Return a friendly error message
		if (error instanceof Error) {
			if (error.message.includes('API key')) {
				throw new Error('AI service not configured. Please add GEMINI_API_KEY to environment.');
			}
			if (error.message.includes('quota') || error.message.includes('limit')) {
				throw new Error('AI service temporarily unavailable. Please try again in a moment.');
			}
		}

		throw new Error('Failed to get AI response. Please try again.');
	}
}

/**
 * Retry logic for AI responses with exponential backoff
 */
export async function getAiResponseWithRetry(
	message: string,
	history: HistoryEntry[],
	systemInstruction: string,
	maxRetries: number = 3
): Promise<string> {
	let lastError: Error | null = null;

	for (let attempt = 0; attempt < maxRetries; attempt++) {
		try {
			return await getAiResponse(message, history, systemInstruction);
		} catch (error) {
			lastError = error instanceof Error ? error : new Error('Unknown error');

			console.log(`AI response attempt ${attempt + 1} failed:`, lastError.message);

			// Don't retry on configuration errors
			if (lastError.message.includes('not configured') || lastError.message.includes('API key')) {
				throw lastError;
			}

			// Wait before retry (exponential backoff)
			if (attempt < maxRetries - 1) {
				const delay = Math.pow(2, attempt) * 1000; // 1s, 2s, 4s
				console.log(`Waiting ${delay}ms before retry...`);
				await new Promise(resolve => setTimeout(resolve, delay));
			}
		}
	}

	// All retries failed
	throw lastError || new Error('Failed to get AI response after multiple attempts');
}

/**
 * Analyze whether the user wants to switch conversation paths
 */
export async function analyzePathSwitchingIntent(
	message: string,
	currentPath: string,
	recentHistory: Array<{ role: string; content: string }>,
	locale: string = 'de'
): Promise<PathSwitchAnalysis> {
	console.log('::analyzePathSwitchingIntent - Received currentPath:', currentPath);
	console.log('::analyzePathSwitchingIntent - User message:', message);
	try {
		const ai = getAiClient();

		const systemPrompt = `Du bist ein Experte f√ºr Gespr√§chsanalyse und Gewaltfreie Kommunikation. Analysiere, ob der Nutzer zu einem anderen Gespr√§chspfad wechseln m√∂chte.

Aktueller Pfad: ${currentPath}

WICHTIG: Der aktuelle Pfad ist "${currentPath}". Verwende GENAU diesen Wert in deiner Analyse!

Verf√ºgbare Pfade:
- idle: Gespr√§chsf√ºhrung (Meta-Ebene, Richtungsvorschl√§ge, noch kein konkretes Thema)
- self_empathy: Selbst-Empathie (eigene Gef√ºhle und Bed√ºrfnisse verstehen)
- other_empathy: Fremd-Empathie (Empathie f√ºr andere Personen entwickeln)
- action_planning: Handlungsplanung (konkrete Schritte planen)
- conflict_resolution: Konfliktl√∂sung (Probleme mit anderen l√∂sen)
- memory: Erinnerungen ABRUFEN (gespeicherte Informationen √ºber den Nutzer anzeigen)
- feedback: Gespr√§ch beenden (Feedback sammeln und Gespr√§ch abschlie√üen)

WICHTIG: Achte auf die ABSICHT des Nutzers, nicht nur auf exakte Keywords!

KRITISCHER UNTERSCHIED:
- "merken" / "merke dir" / "vergiss nicht" = Nutzer m√∂chte etwas SPEICHERN ‚Üí NICHT zu memory wechseln, im aktuellen Pfad bleiben
- "was erinnerst du" / "was wei√üt du" / "erz√§hl mir von" = Nutzer m√∂chte Erinnerungen ABRUFEN ‚Üí zu memory wechseln

FALL A: Nutzer ist im Pfad "idle" (Gespr√§chsf√ºhrung):
‚Üí Wechsle SOFORT, wenn der Nutzer ein konkretes Thema oder Ziel √§u√üert:
  - "Selbst-Empathie" / "meine Gef√ºhle" / "wie ich mich f√ºhle" ‚Üí self_empathy
  - "andere Person" / "jemand anderen" / "Empathie f√ºr X" ‚Üí other_empathy
  - "was tun" / "Handlung" / "Schritte" / "Plan" ‚Üí action_planning
  - "Konflikt" / "Streit" / "Problem l√∂sen" ‚Üí conflict_resolution
  - "was erinnerst du" / "was wei√üt du √ºber mich" / "erz√§hl mir von fr√ºher" ‚Üí memory (NUR zum Abrufen!)
  - "beenden" / "fertig" / "Schluss" ‚Üí feedback
  - "merke dir" / "vergiss nicht" / "merken" ‚Üí NICHT wechseln, im idle bleiben (Speichern passiert automatisch)

FALL B: Nutzer ist bereits in einem spezifischen Pfad (nicht idle):
‚Üí Wechsle nur, wenn der Nutzer EXPLIZIT ein ANDERES Thema nennt
‚Üí Bleibe im aktuellen Pfad, wenn der Nutzer das aktuelle Thema weiter vertieft

Beispiele:

Aktueller Pfad = idle, Nachricht = "ich w√ºrde gerne selbstempathie erhalten"
‚Üí shouldSwitch: true, suggestedPath: "self_empathy", confidence: 95

Aktueller Pfad = idle, Nachricht = "ich habe Stress mit meinem Chef"
‚Üí shouldSwitch: true, suggestedPath: "self_empathy", confidence: 85

Aktueller Pfad = self_empathy, Nachricht = "ich f√ºhle mich traurig"
‚Üí shouldSwitch: false (vertieft das aktuelle Thema)

Aktueller Pfad = self_empathy, Nachricht = "k√∂nnen wir jetzt zur handlungsplanung?"
‚Üí shouldSwitch: true, suggestedPath: "action_planning", confidence: 95

Aktueller Pfad = idle, Nachricht = "kannst du dir merken, dass ich Otter mag?"
‚Üí shouldSwitch: false (Nutzer m√∂chte etwas speichern, nicht abrufen - Speichern passiert automatisch im Hintergrund)

Aktueller Pfad = idle, Nachricht = "was erinnerst du dich √ºber mich?"
‚Üí shouldSwitch: true, suggestedPath: "memory", confidence: 95 (Nutzer m√∂chte Erinnerungen abrufen)

Antworte ausschlie√ülich mit einem JSON-Objekt:
{
  "shouldSwitch": boolean,
  "confidence": 0-100,
  "suggestedPath": "path_id oder null",
  "reason": "kurze Erkl√§rung der Analyse",
  "currentPathComplete": boolean
}`;

		const model = ai.chats.create({
			model: 'gemini-2.5-flash',
			config: {
				temperature: 0.1,
				systemInstruction: systemPrompt
			}
		});

		// Include recent context for better analysis
		const contextMessage = `AKTUELLER PFAD: ${currentPath}

Aktuelle Nachricht des Nutzers: "${message}"

Letzter Gespr√§chsverlauf:
${recentHistory.slice(-4).map(h => `${h.role}: ${h.content}`).join('\n')}

Analysiere diese Nachricht und bestimme, ob der Nutzer vom aktuellen Pfad "${currentPath}" zu einem anderen Pfad wechseln m√∂chte.`;

		const result = await model.sendMessage({ message: contextMessage });
		const responseText = result.text || '{}';

		// Clean the response text
		let cleanedResponseText = responseText.trim();
		if (cleanedResponseText.startsWith('```json')) {
			cleanedResponseText = cleanedResponseText.replace(/^```json\s*/, '').replace(/\s*```$/, '');
		} else if (cleanedResponseText.startsWith('```')) {
			cleanedResponseText = cleanedResponseText.replace(/^```\s*/, '').replace(/\s*```$/, '');
		}

		const analysis = JSON.parse(cleanedResponseText) as PathSwitchAnalysis;
		console.log('üîç Path switching analysis result:');
		console.log('   Current path (from parameter):', currentPath);
		console.log('   Should switch:', analysis.shouldSwitch);
		console.log('   Confidence:', analysis.confidence);
		console.log('   Suggested path:', analysis.suggestedPath);
		console.log('   Reason:', analysis.reason);
		console.log('   Current path complete:', analysis.currentPathComplete);

		return analysis;

	} catch (error) {
		console.error('Error in path switching analysis:', error);
		return {
			shouldSwitch: false,
			confidence: 0,
			suggestedPath: null,
			reason: 'Analysis error',
			currentPathComplete: false
		};
	}
}
